{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Before the text is fed into the classifier, the following steps are taken:\n",
    "\n",
    "1. Detect language\n",
    "2. Translate into English\n",
    "3. Lemmatization\n",
    "4. Remove punctuation but not numbers\n",
    "5. Stop word removal\n",
    "6. Join strings separated by a space\n",
    "7. Converts the string into lower case characters\n",
    "8. Converts everything into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/deepl/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/deepl/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_wordnet_tag_map = {\n",
    "    'NN': wn.NOUN,\n",
    "    'NNS': wn.NOUN,\n",
    "    'VBP': wn.VERB,\n",
    "    'VBG': wn.VERB,\n",
    "    'JJ': wn.ADJ,\n",
    "}\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "def compose(*functions):\n",
    "    def compose2(f1, f2):\n",
    "        \"\"\"Compose two functions\"\"\"\n",
    "        return lambda *args: f1(f2(*args))\n",
    "    return reduce(compose2, functions)\n",
    "\n",
    "def translate_to_english_txt(row):\n",
    "    text = row[\"excerpt\"]\n",
    "    try:\n",
    "        if langid.classify(text)[0] != 'en':\n",
    "            trans = googletrans.client.Translator()\n",
    "            return trans.translate(text, 'en').text\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return ''\n",
    "    \n",
    "def lemmatize(row, lemmatizer=WordNetLemmatizer()):\n",
    "    text = row\n",
    "    splitted = text if type(text) == list else str(text).split()\n",
    "    splitted = list(map(lambda x: str(x).lower(), splitted))\n",
    "    tagged = nltk.pos_tag(splitted)\n",
    "    lemmatized = []\n",
    "    for word, tag in tagged:\n",
    "        wnet_tag = nltk_wordnet_tag_map.get(tag)\n",
    "        if wnet_tag:\n",
    "            lemmatized.append(lemmatizer.lemmatize(word, wnet_tag))\n",
    "        else:\n",
    "            lemmatized.append(word)\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "def rm_punc_not_nums(inp, col=None):\n",
    "    \"\"\"Remove punctuation unless it's a number for either a df (and col)\n",
    "    or single entry\n",
    "    \"\"\"\n",
    "    punc = string.punctuation\n",
    "    transtable = str.maketrans(\"\", \"\", punc)\n",
    "\n",
    "    def sing_rm(phr):\n",
    "        \"\"\"Remove for a single entity\"\"\"\n",
    "        return ' '.join([re.sub('\\W+', '', i).translate(transtable) if not (\n",
    "                    all(j.isdigit() or j in punc for j in i)\n",
    "                    and\n",
    "                    any(j.isdigit() for j in i)\n",
    "                ) else re.sub('\\W+', '', i)\n",
    "                for i in str(phr).split(' ')]\n",
    "        )\n",
    "    if col and isinstance(inp, pd.core.frame.DataFrame):\n",
    "        return inp.filter(like=col).applymap(lambda phr: sing_rm(phr))\n",
    "    elif isinstance(inp, str):\n",
    "        return sing_rm(inp)\n",
    "    else:\n",
    "        raise Exception('Not a vaild type')\n",
    "\n",
    "\n",
    "def rm_stop_words_txt(txt, swords=nltk.corpus.stopwords.words('english')):\n",
    "    \"\"\" Remove stop words from given text \"\"\"\n",
    "    return ' '.join(\n",
    "        [token for token in str(txt).split(' ')\n",
    "            if token.lower() not in swords]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 quick brown fox jumped lazy dogs'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"excerpt\": \"The 2 quick brown foxes jumped over the lazy dogs!\"}\n",
    "\n",
    "def preprocess(row):\n",
    "        inp = row[\"excerpt\"]\n",
    "        inp = lemmatize(inp)\n",
    "        func = compose(\n",
    "            rm_punc_not_nums,\n",
    "            rm_stop_words_txt,\n",
    "            ' '.join,\n",
    "            str.split,\n",
    "            str.lower,\n",
    "            str\n",
    "        )\n",
    "        \n",
    "        return func(inp)\n",
    "\n",
    "preprocess(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/all_en_processed_sectors_subsectors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"Unnamed: 0\", \"Unnamed: 0.1\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"preprocessed_excerpt\"] = df.apply(preprocess, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df=None\n",
    "df = pd.DataFrame(df.groupby('preprocessed_excerpt')[\"sector\"].apply(set)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [x.strip() for x in df['preprocessed_excerpt'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([4]), list([9]), list([3]), ..., list([5]), list([3]),\n",
       "       list([3])], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten list\n",
    "temp_list = [list(x) for x in list(df[\"sector\"])]\n",
    "category_columns = [j for sub in temp_list for j in sub]\n",
    "category_columns = list(set(category_columns))\n",
    "category_columns.sort()\n",
    "\n",
    "labels_text={}\n",
    "for i, x in enumerate(category_columns):\n",
    "    labels_text[x]=i\n",
    "    \n",
    "labels_text\n",
    "\n",
    "labels=[]\n",
    "for x in temp_list:\n",
    "    labels.append([labels_text[y] for y in x])\n",
    "labels=np.array(labels)   \n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed_with_vectors.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = \n",
    "dfgroup1=pd.DataFrame(df.groupby('preprocessed_excerpt')[\"sector\"].apply(set)).reset_index()\n",
    "dfgroup2=pd.DataFrame(df.groupby('preprocessed_excerpt')[\"feature_vector\"].apply(list)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors=[]\n",
    "labels=[]\n",
    "for item1_i, item1 in dfgroup1.iterrows():\n",
    "    item2 = dfgroup2.iloc[item1_i]\n",
    "    vector=[x.replace('[','').replace(']','').replace('\\n','').split(' ') for x in item2['feature_vector']]\n",
    "    vectors.append([float(x.strip()) for x in vector[0] if x.strip()!=''])\n",
    "    labels.append([labels_text[y] for y in item1['sector']])\n",
    "labels=np.array(labels)  \n",
    "vectors=np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump([vectors, labels], open('vectors_doc2vec_dim50.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 20000 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "MAX_NB_WORDS=20000\n",
    "\n",
    "dictionary = Dictionary([x.split() for x in documents])\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.5, keep_n=MAX_NB_WORDS)\n",
    "dictionary.compactify()\n",
    "print('Total %s unique tokens.' % len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(line) for line in [x.split() for x in documents]]  # convert corpus to BoW format\n",
    "\n",
    "model = TfidfModel(corpus)  # fit model\n",
    "vectors_tfidf=[]\n",
    "for x in corpus:\n",
    "    vectors_tfidf.append(model[x])\n",
    "corpus = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_tfidf_sparse=np.zeros((len(vectors_tfidf),MAX_NB_WORDS))\n",
    "for doc_i, doc in enumerate(vectors_tfidf):\n",
    "    for item in doc:\n",
    "        vectors_tfidf_sparse[doc_i][item[0]]=item[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump([dictionary, labels_text], open('dictionary_labels_text.pkl', 'wb'))\n",
    "pickle.dump([vectors_tfidf_sparse, labels], open('vectors_tfidf_sparse.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add categories as columns\n",
    "#category_df = pd.concat([merged_df,pd.DataFrame(columns=category_columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#category_df.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def update(row):\n",
    "#    for each in list(row[\"sector\"]):\n",
    "#        row[each] = 1\n",
    "#    return row\n",
    "#category_df = category_df.apply(update, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "text_clf = Pipeline([\n",
    "            ('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "            ('tfidf', TfidfTransformer(use_idf=False))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = text_clf.fit_transform(category_df[\"preprocessed_excerpt\"])\n",
    "labels = category_df.iloc[:,:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cv(data, labels, model):\n",
    "    print (\"Cross Validating: {0}\".format(model[\"name\"]))\n",
    "    print (\"Data shape\", data.shape)\n",
    "    print (\"Labels shape\", labels.shape)\n",
    "    scoring = ['accuracy', 'precision_macro', 'recall_macro']\n",
    "    scores = cross_validate(model[\"model\"], data, labels, scoring=scoring, cv=ms.KFold(n_splits=2, shuffle = True, random_state=7), return_train_score=False)\n",
    "    k = list(range(10))\n",
    "    plt.bar(k, scores['test_accuracy'])\n",
    "    plt.bar(k, scores['test_precision_macro'])\n",
    "    plt.bar(k, scores['test_recall_macro'])\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.title(\"{0} 10 Folds Cross Validation\".format(model[\"name\"]))\n",
    "    plt.legend(['Testing Accuracy', 'Testing Precision', 'Testing Recall'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validating: Ridge Classifier - Exponential Loss\n",
      "Data shape (15852, 376429)\n",
      "Labels shape (15852, 12)\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "#     {\n",
    "#         \"name\": \"MLP\",\n",
    "#         \"model\": MLPClassifier(activation='logistic', learning_rate='adaptive', verbose=True, early_stopping=True)\n",
    "#     },\n",
    "    {\n",
    "        \"name\": \"Ridge Classifier - Exponential Loss\",\n",
    "        \"model\": RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1])\n",
    "    },\n",
    "#     {\n",
    "#         \"name\": \"Gradient Boosting Classifier - Deviance Loss\",\n",
    "#         \"model\": GradientBoostingClassifier(loss=\"deviance\", n_estimators=300)\n",
    "#     },\n",
    "    \n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model_cv(features, labels, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
