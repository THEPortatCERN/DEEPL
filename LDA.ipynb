{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.helpers.common import compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONAL HELPERS\n",
    "def curry2(func):\n",
    "    \"\"\"Curry the two arguments function\"\"\"\n",
    "    def f1(x):\n",
    "        def f2(y):\n",
    "            return func(x, y)\n",
    "        return f2\n",
    "    return f1\n",
    "\n",
    "# CURRY our map and filter\n",
    "curried_filter = curry2(filter)\n",
    "curried_map = curry2(map)\n",
    "\n",
    "map_lower = curried_map(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# STOP WORDS\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "# function that returns true if not stop word\n",
    "not_stop = lambda x: x not in en_stop\n",
    "\n",
    "# STEMMING\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "# map stem\n",
    "map_stem = curried_map(stemmer.stem)\n",
    "\n",
    "# Create a pre-processor function\n",
    "# First tokenize, lowercase, then filter nonstop and then stem.\n",
    "# Result will be a map. convert to list\n",
    "tokenize_remove_stop_stem = compose(list, map_stem, curried_filter(not_stop), map_lower, tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['brocolli', 'good', 'eat', 'brother', 'like', 'eat', 'good', 'brocolli', 'mother'], ['mother', 'spend', 'lot', 'time', 'drive', 'brother', 'around', 'basebal', 'practic'], ['health', 'expert', 'suggest', 'drive', 'may', 'caus', 'increas', 'tension', 'blood', 'pressur'], ['often', 'feel', 'pressur', 'perform', 'well', 'school', 'mother', 'never', 'seem', 'drive', 'brother', 'better'], ['health', 'profession', 'say', 'brocolli', 'good', 'health']]\n"
     ]
    }
   ],
   "source": [
    "# Create our processed texts\n",
    "texts = [tokenize_remove_stop_stem(x) for x in doc_set]\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_cache', '_abc_negative_cache', '_abc_negative_cache_version', '_abc_registry', '_adapt_by_suffix', '_load_specials', '_save_specials', '_smart_save', 'add_documents', 'compactify', 'dfs', 'doc2bow', 'filter_extremes', 'filter_n_most_frequent', 'filter_tokens', 'from_corpus', 'from_documents', 'get', 'id2token', 'items', 'iteritems', 'iterkeys', 'itervalues', 'keys', 'load', 'load_from_text', 'merge_with', 'num_docs', 'num_nnz', 'num_pos', 'save', 'save_as_text', 'token2id', 'values']\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "# CONSTRUCTING A DOCUMENT TERM MATRIX\n",
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dir(dictionary))\n",
    "print(dictionary.get(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1)]\n"
     ]
    }
   ],
   "source": [
    "# CONVERT TO BAG OF WORDS\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(corpus[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.082*\"brother\" + 0.082*\"mother\"')\n",
      "(1, '0.141*\"health\" + 0.080*\"profession\"')\n",
      "(2, '0.072*\"drive\" + 0.071*\"pressur\"')\n"
     ]
    }
   ],
   "source": [
    "# APPLY LDA\n",
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)\n",
    "for x in ldamodel.print_topics(num_topics=3, num_words=2):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_adapt_by_suffix', '_apply', '_load_specials', '_save_specials', '_smart_save', 'alpha', 'bound', 'callbacks', 'chunksize', 'clear', 'decay', 'diff', 'dispatcher', 'distributed', 'do_estep', 'do_mstep', 'eta', 'eval_every', 'expElogbeta', 'gamma_threshold', 'get_document_topics', 'get_term_topics', 'get_topic_terms', 'get_topics', 'id2word', 'inference', 'init_dir_prior', 'iterations', 'load', 'log_perplexity', 'minimum_phi_value', 'minimum_probability', 'num_terms', 'num_topics', 'num_updates', 'numworkers', 'offset', 'optimize_alpha', 'optimize_eta', 'passes', 'per_word_topics', 'print_topic', 'print_topics', 'random_state', 'save', 'show_topic', 'show_topics', 'state', 'sync_state', 'top_topics', 'update', 'update_alpha', 'update_eta', 'update_every']\n"
     ]
    }
   ],
   "source": [
    "print(dir(ldamodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.082*\"brother\" + 0.082*\"mother\" + 0.081*\"brocolli\" + 0.081*\"good\" + 0.081*\"eat\" + 0.046*\"basebal\" + 0.046*\"lot\" + 0.046*\"practic\" + 0.046*\"around\" + 0.046*\"spend\" + 0.046*\"time\" + 0.046*\"like\" + 0.046*\"drive\" + 0.012*\"health\" + 0.012*\"increas\" + 0.012*\"tension\" + 0.012*\"pressur\" + 0.012*\"expert\" + 0.012*\"may\" + 0.012*\"caus\" + 0.012*\"suggest\" + 0.012*\"blood\" + 0.012*\"feel\" + 0.012*\"perform\" + 0.012*\"better\" + 0.012*\"often\" + 0.012*\"well\" + 0.012*\"seem\" + 0.012*\"never\" + 0.012*\"school\" + 0.012*\"say\" + 0.012*\"profession\"'), (1, '0.141*\"health\" + 0.080*\"profession\" + 0.080*\"say\" + 0.080*\"brocolli\" + 0.080*\"good\" + 0.020*\"drive\" + 0.020*\"mother\" + 0.020*\"brother\" + 0.020*\"increas\" + 0.020*\"may\" + 0.020*\"tension\" + 0.020*\"expert\" + 0.020*\"caus\" + 0.020*\"suggest\" + 0.020*\"blood\" + 0.020*\"pressur\" + 0.020*\"like\" + 0.020*\"eat\" + 0.020*\"perform\" + 0.020*\"seem\" + 0.020*\"never\" + 0.020*\"school\" + 0.020*\"time\" + 0.020*\"better\" + 0.020*\"spend\" + 0.020*\"around\" + 0.020*\"practic\" + 0.020*\"lot\" + 0.020*\"basebal\" + 0.020*\"often\" + 0.020*\"feel\" + 0.020*\"well\"'), (2, '0.072*\"drive\" + 0.071*\"pressur\" + 0.041*\"well\" + 0.041*\"often\" + 0.041*\"feel\" + 0.041*\"school\" + 0.041*\"better\" + 0.041*\"never\" + 0.041*\"seem\" + 0.041*\"perform\" + 0.041*\"blood\" + 0.041*\"suggest\" + 0.041*\"caus\" + 0.041*\"may\" + 0.041*\"expert\" + 0.041*\"tension\" + 0.041*\"increas\" + 0.041*\"mother\" + 0.041*\"brother\" + 0.040*\"health\" + 0.010*\"good\" + 0.010*\"brocolli\" + 0.010*\"spend\" + 0.010*\"time\" + 0.010*\"around\" + 0.010*\"basebal\" + 0.010*\"lot\" + 0.010*\"practic\" + 0.010*\"like\" + 0.010*\"eat\" + 0.010*\"profession\" + 0.010*\"say\"')]\n",
      "\n",
      "[[ 0.08140385  0.08140371  0.08133688  0.08159296  0.046466    0.08159264\n",
      "   0.04646791  0.04646805  0.04646788  0.04628699  0.04646802  0.04646808\n",
      "   0.04646805  0.01164948  0.01164188  0.01163995  0.01164109  0.01164064\n",
      "   0.01164304  0.01164227  0.01163992  0.01164203  0.01163288  0.01163292\n",
      "   0.0116329   0.01163288  0.01163284  0.01163287  0.01163288  0.0116329\n",
      "   0.01163278  0.01163282]\n",
      " [ 0.07975153  0.07975117  0.01999245  0.02001622  0.01999403  0.02001667\n",
      "   0.01998903  0.01998896  0.01998908  0.02002044  0.01998899  0.0199889\n",
      "   0.01998898  0.14099503  0.01999724  0.0199972   0.01999733  0.01999722\n",
      "   0.01999736  0.01999729  0.01999719  0.01999581  0.01998886  0.01998885\n",
      "   0.01999031  0.01998873  0.01998912  0.01998927  0.01998939  0.01998907\n",
      "   0.07981914  0.07981912]\n",
      " [ 0.01023056  0.01023087  0.01022452  0.04064015  0.01022481  0.04064021\n",
      "   0.01022569  0.01022561  0.01022569  0.0716745   0.01022562  0.01022562\n",
      "   0.0102256   0.04020188  0.04083609  0.04083781  0.04083674  0.04083719\n",
      "   0.040835    0.04083571  0.04083784  0.07148961  0.04084829  0.04084826\n",
      "   0.04084753  0.04084835  0.04084818  0.04084809  0.04084802  0.04084816\n",
      "   0.01022392  0.0102239 ]]\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.show_topics(num_words=40)\n",
    "topics1 = ldamodel.get_topics()\n",
    "print(topics)\n",
    "print()\n",
    "print(topics1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.082*\"brother\" + 0.082*\"mother\"'), (1, '0.031*\"brocolli\" + 0.031*\"good\"'), (2, '0.125*\"health\" + 0.050*\"pressur\"')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('brother', 0.081972768357628872), ('mother', 0.081972667651560513)],\n",
       " [('brocolli', 0.031347669001034882), ('good', 0.031347571204249164)],\n",
       " [('health', 0.12501218829869792), ('pressur', 0.050117218597194459)]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from core.topic_modeling.lda import LDAModel\n",
    "\n",
    "lda = LDAModel()\n",
    "\n",
    "model = lda.create_model(doc_set, 3)\n",
    "print(model.print_topics(num_words=2))\n",
    "lda.get_topics_and_keywords(num_words=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (.env)",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
