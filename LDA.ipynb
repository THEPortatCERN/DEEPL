{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.helpers.common import compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONAL HELPERS\n",
    "def curry2(func):\n",
    "    \"\"\"Curry the two arguments function\"\"\"\n",
    "    def f1(x):\n",
    "        def f2(y):\n",
    "            return func(x, y)\n",
    "        return f2\n",
    "    return f1\n",
    "\n",
    "# CURRY our map and filter\n",
    "curried_filter = curry2(filter)\n",
    "curried_map = curry2(map)\n",
    "\n",
    "map_lower = curried_map(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# STOP WORDS\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "# function that returns true if not stop word\n",
    "not_stop = lambda x: x not in en_stop\n",
    "\n",
    "# STEMMING\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "# map stem\n",
    "map_stem = curried_map(stemmer.stem)\n",
    "\n",
    "# Create a pre-processor function\n",
    "# First tokenize, lowercase, then filter nonstop and then stem.\n",
    "# Result will be a map. convert to list\n",
    "tokenize_remove_stop_stem = compose(list, map_stem, curried_filter(not_stop), map_lower, tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['brocolli', 'good', 'eat', 'brother', 'like', 'eat', 'good', 'brocolli', 'mother'], ['mother', 'spend', 'lot', 'time', 'drive', 'brother', 'around', 'basebal', 'practic'], ['health', 'expert', 'suggest', 'drive', 'may', 'caus', 'increas', 'tension', 'blood', 'pressur'], ['often', 'feel', 'pressur', 'perform', 'well', 'school', 'mother', 'never', 'seem', 'drive', 'brother', 'better'], ['health', 'profession', 'say', 'brocolli', 'good', 'health']]\n"
     ]
    }
   ],
   "source": [
    "# Create our processed texts\n",
    "texts = [tokenize_remove_stop_stem(x) for x in doc_set]\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(32 unique tokens: ['brocolli', 'good', 'eat', 'brother', 'like']...)\n"
     ]
    }
   ],
   "source": [
    "# CONSTRUCTING A DOCUMENT TERM MATRIX\n",
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1)]\n"
     ]
    }
   ],
   "source": [
    "# CONVERT TO BAG OF WORDS\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(corpus[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.074*\"mother\" + 0.074*\"brother\"')\n",
      "(1, '0.130*\"good\" + 0.130*\"brocolli\"')\n",
      "(2, '0.065*\"pressur\" + 0.065*\"health\"')\n"
     ]
    }
   ],
   "source": [
    "# APPLY LDA\n",
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)\n",
    "for x in ldamodel.print_topics(num_topics=3, num_words=2):\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (.env)",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
