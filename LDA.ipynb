{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.common import compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONAL HELPERS\n",
    "def curry2(func):\n",
    "    \"\"\"Curry the two arguments function\"\"\"\n",
    "    def f1(x):\n",
    "        def f2(y):\n",
    "            return func(x, y)\n",
    "        return f2\n",
    "    return f1\n",
    "\n",
    "# CURRY our map and filter\n",
    "curried_filter = curry2(filter)\n",
    "curried_map = curry2(map)\n",
    "\n",
    "map_lower = curried_map(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# STOP WORDS\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "# function that returns true if not stop word\n",
    "not_stop = lambda x: x not in en_stop\n",
    "\n",
    "# STEMMING\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "# map stem\n",
    "map_stem = curried_map(stemmer.stem)\n",
    "\n",
    "# Create a pre-processor function\n",
    "# First tokenize, lowercase, then filter nonstop and then stem.\n",
    "# Result will be a map. convert to list\n",
    "tokenize_remove_stop_stem = compose(list, map_stem, curried_filter(not_stop), map_lower, tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['brocolli', 'good', 'eat', 'brother', 'like', 'eat', 'good', 'brocolli', 'mother'], ['mother', 'spend', 'lot', 'time', 'drive', 'brother', 'around', 'basebal', 'practic'], ['health', 'expert', 'suggest', 'drive', 'may', 'caus', 'increas', 'tension', 'blood', 'pressur'], ['often', 'feel', 'pressur', 'perform', 'well', 'school', 'mother', 'never', 'seem', 'drive', 'brother', 'better'], ['health', 'profession', 'say', 'brocolli', 'good', 'health']]\n"
     ]
    }
   ],
   "source": [
    "# Create our processed texts\n",
    "texts = [tokenize_remove_stop_stem(x) for x in doc_set]\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_cache', '_abc_negative_cache', '_abc_negative_cache_version', '_abc_registry', '_adapt_by_suffix', '_load_specials', '_save_specials', '_smart_save', 'add_documents', 'compactify', 'dfs', 'doc2bow', 'filter_extremes', 'filter_n_most_frequent', 'filter_tokens', 'from_corpus', 'from_documents', 'get', 'id2token', 'items', 'iteritems', 'iterkeys', 'itervalues', 'keys', 'load', 'load_from_text', 'merge_with', 'num_docs', 'num_nnz', 'num_pos', 'save', 'save_as_text', 'token2id', 'values']\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "# CONSTRUCTING A DOCUMENT TERM MATRIX\n",
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dir(dictionary))\n",
    "print(dictionary.get(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1)]\n"
     ]
    }
   ],
   "source": [
    "# CONVERT TO BAG OF WORDS\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(corpus[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.120*\"good\" + 0.120*\"brocolli\"')\n",
      "(1, '0.074*\"drive\" + 0.074*\"brother\"')\n",
      "(2, '0.125*\"health\" + 0.050*\"pressur\"')\n"
     ]
    }
   ],
   "source": [
    "# APPLY LDA\n",
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)\n",
    "for x in ldamodel.print_topics(num_topics=3, num_words=2):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_adapt_by_suffix', '_apply', '_load_specials', '_save_specials', '_smart_save', 'alpha', 'bound', 'callbacks', 'chunksize', 'clear', 'decay', 'diff', 'dispatcher', 'distributed', 'do_estep', 'do_mstep', 'eta', 'eval_every', 'expElogbeta', 'gamma_threshold', 'get_document_topics', 'get_term_topics', 'get_topic_terms', 'get_topics', 'id2word', 'inference', 'init_dir_prior', 'iterations', 'load', 'log_perplexity', 'minimum_phi_value', 'minimum_probability', 'num_terms', 'num_topics', 'num_updates', 'numworkers', 'offset', 'optimize_alpha', 'optimize_eta', 'passes', 'per_word_topics', 'print_topic', 'print_topics', 'random_state', 'save', 'show_topic', 'show_topics', 'state', 'sync_state', 'top_topics', 'update', 'update_alpha', 'update_eta', 'update_every']\n",
      "[(0, 0.93099914705528453), (1, 0.034901713872807646), (2, 0.034099139071907945)]\n"
     ]
    }
   ],
   "source": [
    "print(dir(ldamodel))\n",
    "print(ldamodel.get_document_topics(corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.121*\"brocolli\" + 0.121*\"good\" + 0.118*\"eat\" + 0.068*\"brother\" + 0.068*\"mother\" + 0.067*\"like\" + 0.017*\"health\" + 0.017*\"profession\" + 0.017*\"say\" + 0.017*\"drive\" + 0.017*\"pressur\" + 0.017*\"tension\" + 0.017*\"suggest\" + 0.017*\"increas\" + 0.017*\"expert\" + 0.017*\"caus\" + 0.017*\"around\" + 0.017*\"practic\" + 0.017*\"spend\" + 0.017*\"basebal\" + 0.017*\"time\" + 0.017*\"lot\" + 0.017*\"may\" + 0.017*\"blood\" + 0.017*\"never\" + 0.017*\"feel\" + 0.017*\"perform\" + 0.017*\"seem\" + 0.017*\"better\" + 0.017*\"school\" + 0.017*\"often\" + 0.017*\"well\"'), (1, '0.082*\"health\" + 0.047*\"pressur\" + 0.047*\"mother\" + 0.047*\"brother\" + 0.047*\"well\" + 0.047*\"often\" + 0.047*\"school\" + 0.047*\"seem\" + 0.047*\"better\" + 0.047*\"perform\" + 0.047*\"never\" + 0.047*\"feel\" + 0.047*\"say\" + 0.047*\"profession\" + 0.047*\"drive\" + 0.044*\"good\" + 0.044*\"brocolli\" + 0.012*\"eat\" + 0.012*\"like\" + 0.012*\"tension\" + 0.012*\"caus\" + 0.012*\"suggest\" + 0.012*\"increas\" + 0.012*\"blood\" + 0.012*\"may\" + 0.012*\"expert\" + 0.012*\"spend\" + 0.012*\"practic\" + 0.012*\"around\" + 0.012*\"time\" + 0.012*\"lot\" + 0.012*\"basebal\"'), (2, '0.079*\"drive\" + 0.045*\"lot\" + 0.045*\"basebal\" + 0.045*\"time\" + 0.045*\"around\" + 0.045*\"spend\" + 0.045*\"practic\" + 0.045*\"expert\" + 0.045*\"may\" + 0.045*\"blood\" + 0.045*\"increas\" + 0.045*\"caus\" + 0.045*\"suggest\" + 0.045*\"tension\" + 0.045*\"pressur\" + 0.045*\"health\" + 0.045*\"mother\" + 0.045*\"brother\" + 0.011*\"brocolli\" + 0.011*\"good\" + 0.011*\"say\" + 0.011*\"profession\" + 0.011*\"feel\" + 0.011*\"better\" + 0.011*\"perform\" + 0.011*\"seem\" + 0.011*\"never\" + 0.011*\"well\" + 0.011*\"school\" + 0.011*\"often\" + 0.011*\"like\" + 0.011*\"eat\"')]\n",
      "\n",
      "[[ 0.1209042   0.12088918  0.11766694  0.06764345  0.06723297  0.06764093\n",
      "   0.01684028  0.01684016  0.01684025  0.01686013  0.01684035  0.01684025\n",
      "   0.01684033  0.01691913  0.01684064  0.01684191  0.01684007  0.01684056\n",
      "   0.0168413   0.01684207  0.01683971  0.01685759  0.01683278  0.01683285\n",
      "   0.01683284  0.01683276  0.0168328   0.01683288  0.01683281  0.0168328\n",
      "   0.01689758  0.01689747]\n",
      " [ 0.04448949  0.04449994  0.01170998  0.04671649  0.01169828  0.0467179\n",
      "   0.01169144  0.01169136  0.01169138  0.04657534  0.01169139  0.01169135\n",
      "   0.01169142  0.08187847  0.01169396  0.01169599  0.01169515  0.01169634\n",
      "   0.0116955   0.01169752  0.01169549  0.04676189  0.0467037   0.0467036\n",
      "   0.04670362  0.0467037   0.04670368  0.04670362  0.04670365  0.04670364\n",
      "   0.04665232  0.04665239]\n",
      " [ 0.01126505  0.01126502  0.01125156  0.0447366   0.01125167  0.04473693\n",
      "   0.04495596  0.04495612  0.04495604  0.07883141  0.04495596  0.04495607\n",
      "   0.04495595  0.04480415  0.04495329  0.04495049  0.04495253  0.04495106\n",
      "   0.04495137  0.04494891  0.04495244  0.04491705  0.01125319  0.01125324\n",
      "   0.01125322  0.0112532   0.0112532   0.0112532   0.01125321  0.01125324\n",
      "   0.01125932  0.01125933]]\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.show_topics(num_words=40)\n",
    "topics1 = ldamodel.get_topics()\n",
    "print(topics)\n",
    "print()\n",
    "print(topics1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('pressur', 0.06462822782686807), ('health', 0.064537432068554529)], [('brother', 0.073823833351888926), ('mother', 0.073821879207508184)], [('good', 0.12988932077895299), ('brocolli', 0.12988898877140737)]]\n"
     ]
    }
   ],
   "source": [
    "from topic_modeling.lda import LDAModel\n",
    "\n",
    "lda = LDAModel()\n",
    "\n",
    "lda.create_model(doc_set, 3)\n",
    "print(lda.get_topics_and_keywords(num_words=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (.env)",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
