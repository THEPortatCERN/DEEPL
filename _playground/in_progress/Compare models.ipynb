{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/bibek/projects/DEEPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.deep import get_deep_data, get_classifier\n",
    "#data = get_deep_data(debug=False, filepath='/home/bibek/projects/DEEPL/_playground/sample_data/nlp_out.csv')\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/bibek/projects/DEEPL/_playground/sample_data/processed_sectors_subsectors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.common import (\n",
    "    rm_punc_not_nums, rm_punc_not_nums_list,\n",
    "    rm_stop_words_txt, rm_stop_words_txt_list,\n",
    "    remove_punc_and_nums,\n",
    "    translate_to_english_txt,\n",
    "    compose\n",
    ")\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import random\n",
    "import langid\n",
    "\n",
    "from topic_modeling.keywords_extraction import get_key_ngrams\n",
    "\n",
    "from helpers.functional import curried_map, curried_filter, curried_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          [NN, children, five, risk, acute, malnutrition]\n",
      "1        [NN, cholera, cases, suspected, NN-NN, october...\n",
      "2             [NN, people, need, urgent, food, assistance]\n",
      "3                      [NN, health, facilities, destroyed]\n",
      "4                         [NN, million, people, need, aid]\n",
      "5        [humanitarian, needs, said, include, access, s...\n",
      "6        [people, urgent, food, insecurity, located, ar...\n",
      "7        [following, government’s, announcement, close,...\n",
      "8        [exacerbating, pre-existing, displacement, cri...\n",
      "9        [agencies, expressed, serious, concerns, risk,...\n",
      "10       [humanitarian, country, team, hct, haiti, coor...\n",
      "11       [in-kind, contributions, also, made, extend, s...\n",
      "12       [NN, people, extreme, food, insecurity, affect...\n",
      "13       [addition, NN, disbursed, emergency, funds, tr...\n",
      "14       [NN, NN, october, wfp, reached, NN, people, NN...\n",
      "15       [estimated, NN, people, expected, need, vaccin...\n",
      "16       [nearly, NN, children, age, five, risk, acute,...\n",
      "17       [around, NN, suspected, cases, cholera, report...\n",
      "18       [concerns, unsolicited, donations, breast-milk...\n",
      "19       [restoring, basic, health, nutrition, services...\n",
      "20       [estimated, NN, children, school, number, like...\n",
      "21       [estimated, NN, people, grand’anse, sud, targe...\n",
      "22       [on-, five, hospitals, one, mobile, clinic, at...\n",
      "23       [second, hospital, western, aleppo, hit, airst...\n",
      "24       [third, hospital, western, aleppo, hit, airstr...\n",
      "25       [one, damaged, facilities, western, rural, ale...\n",
      "26       [two, hospitals, idleb, providing, almost, NN,...\n",
      "27       [since, july, NN, civilians, trapped, eastern,...\n",
      "28       [gender, inequality, cause, consequence, viole...\n",
      "29       [third, women, experience, violence, point, li...\n",
      "                               ...                        \n",
      "16436    [cross, river, records, NN, deaths, NN, months...\n",
      "16437    [according, recent, wfp, analysis, food, consu...\n",
      "16438    [addition, government, cameroon, ministry, agr...\n",
      "16439    [el, titular, de, carbap, matías, de, velazco,...\n",
      "16440    [residents, however, presence, armed, forces, ...\n",
      "16441    [government, tanzania, said, tuesday, soon, st...\n",
      "16442    [NN, maasai, huts, tanzania, allegedly, burned...\n",
      "16443    [dr, abdul’aziz, manga, executive, chairman, b...\n",
      "16444    [vulnerable, areas, still, regent, kamayama, d...\n",
      "16445    [government, officials, partners, also, active...\n",
      "16446    [pronostican, incremento, de, precipitaciones,...\n",
      "16447    [close, NN, children, age, five, suffering, se...\n",
      "16448    [close, NN, children, age, five, suffering, se...\n",
      "16449    [hawija, NN, besieged, civilians, militant, gr...\n",
      "16450                                                [nan]\n",
      "16451    [NN, cholera, dead, south, darfur, capitaltwo,...\n",
      "16452    [food, security, outlook, overall, high, food,...\n",
      "16453    [yemen’s, cholera, outbreak, infected, NN, peo...\n",
      "16454                                                [nan]\n",
      "16455    [préfecture, de, nana-gribizi, sous-préfecture...\n",
      "16456    [préfecture, de, l’ouham, sous-préfecture, de,...\n",
      "16457    [myanmar, laying, landmines, across, section, ...\n",
      "16458    [les, prix, de, denrées, alimentaires, sont, e...\n",
      "16459    [la, flambée, de, prix, des, denrées, alimenta...\n",
      "16460    [bouca, depuis, quelques, mois, est, la, plaqu...\n",
      "16461    [food, availability, accessibility, improved, ...\n",
      "16462    [sur, les, dix, régions, que, compte, le, pays...\n",
      "16463    [quelques, élèves, ont, été, aperçus, lundi, N...\n",
      "16464    [la, situation, est, calme, depuis, mardi, NN,...\n",
      "16465    [zémio, à, plus, de, NN, kilomètres, de, bangu...\n",
      "Name: excerpt, Length: 16466, dtype: object\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0e89e875a3bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'excerpt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunc_nums_preprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# keywords pre processor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "### COMPARE KEYWORDS EXTRACTION VS SIMPLE method\n",
    "#################################################\n",
    "# create a function to extract keywords from document(will return list)\n",
    "def keywords_extracter(doc):\n",
    "    ngrams = get_key_ngrams(doc, 3)\n",
    "    allwords = {}\n",
    "    for x in ngrams['1grams']: \n",
    "        allwords[x[0]] = True\n",
    "    for y in ngrams['2grams']:\n",
    "        a,b = y[0].split()\n",
    "        allwords[a] = True\n",
    "        allwords[b] = True\n",
    "    for y in ngrams['3grams']:\n",
    "        a,b,c = y[0].split()\n",
    "        allwords[a] = True\n",
    "        allwords[b] = True\n",
    "        allwords[c] = True\n",
    "    return list(allwords.keys())\n",
    "\n",
    "# convert doc to str and then split\n",
    "str_split = compose(str.split, str)\n",
    "# stemmer\n",
    "stemmer = PorterStemmer()\n",
    "# split and stem\n",
    "#split_and_stem = lambda x: list(map(stemmer.stem,x))\n",
    "\n",
    "\n",
    "rm_stop_list = curried_map(rm_stop_words_txt)\n",
    "rm_punc_list = curried_map(rm_punc_not_nums)\n",
    "rm_punc_nums_list = curried_map(remove_punc_and_nums)\n",
    "lower_list = curried_map(str.lower)\n",
    "\n",
    "composed_list_processor= compose(remove_punc_and_nums, rm_stop_words_txt, str.lower)\n",
    "\n",
    "\n",
    "# rm_punc_nums_processor\n",
    "punc_nums_preprocessor = compose(list, curried_filter(lambda x: x.strip()!=''), curried_map(composed_list_processor), str.split, str)\n",
    "\n",
    "processed = df['excerpt'].apply(punc_nums_preprocessor)\n",
    "print(processed)\n",
    "assert False\n",
    "\n",
    "# keywords pre processor\n",
    "kw_preprocessor = compose(list, rm_punc_list , rm_stop_list,lower_list,keywords_extracter,str)\n",
    "\n",
    "# simple pre processor\n",
    "simple_preprocessor = compose(list, rm_punc_list , rm_stop_list,lower_list,str.split,str)\n",
    "\n",
    "kw_processed = [(kw_preprocessor(ex), l) for (ex, l) in data]  # if langid.classify(str(ex))[0] == 'en']\n",
    "simple_processed = [(simple_preprocessor(ex), l) for (ex, l) in data]  # if langid.classify(str(ex))[0] == 'en']\n",
    "punc_nums_processed = [(punc_nums_preprocessor(ex), l) for (ex, l) in data]  # if langid.classify(str(ex))[0] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier.feature_selectors import UnigramFeatureSelector, BigramFeatureSelector\n",
    "from classifier.NaiveBayes_classifier import NaiveBayesClassifier\n",
    "\n",
    "#processed_data = kw_processed\n",
    "#processed_data = simple_processed\n",
    "\n",
    "def get_avg_accuracy(iters, size, processed_data, feature_selector=UnigramFeatureSelector):\n",
    "    sum_accuracy = 0\n",
    "    accuracies = []\n",
    "    for x in range(iters):\n",
    "\n",
    "        random.shuffle(processed_data)\n",
    "        processed_data = processed_data[:size]\n",
    "\n",
    "        data_len = len(processed_data)\n",
    "        test_len = int(data_len * 0.25)\n",
    "\n",
    "        train_data = processed_data[test_len:]\n",
    "        test_data = processed_data[:test_len]\n",
    "\n",
    "        selector = feature_selector.new(corpus=processed_data, top=2000) # use top 2000 words\n",
    "\n",
    "        classifier = NaiveBayesClassifier.new(selector, train_data)\n",
    "        \n",
    "        accuracy = classifier.get_accuracy(test_data)\n",
    "        accuracies.append(accuracy)\n",
    "        sum_accuracy += accuracy\n",
    "    return sum_accuracy/iters, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-7a4cbf965172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpunc_nums_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_avg_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpunc_nums_processed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msimple_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_avg_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_processed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msimple_bigram_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_avg_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_processed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBigramFeatureSelector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'KEYWORDS:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-e6c6cfe854d3>\u001b[0m in \u001b[0;36mget_avg_accuracy\u001b[0;34m(iters, size, processed_data, feature_selector)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessed_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# use top 2000 words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/DEEPL/classifier/feature_selectors.py\u001b[0m in \u001b[0;36mnew\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m'corpus'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# create bigrams from the corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mdoc_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corpus'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# counter for bigrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/DEEPL/classifier/feature_selectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m'corpus'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# create bigrams from the corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mdoc_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corpus'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# counter for bigrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "SIZE = 200\n",
    "ITERS = 2\n",
    "\n",
    "kw_accuracy = get_avg_accuracy(ITERS, SIZE, kw_processed)\n",
    "punc_nums_accuracy = get_avg_accuracy(ITERS, SIZE, punc_nums_processed)\n",
    "simple_accuracy = get_avg_accuracy(ITERS, SIZE, simple_processed)\n",
    "simple_bigram_accuracy = get_avg_accuracy(ITERS, SIZE, simple_processed, BigramFeatureSelector)\n",
    "\n",
    "print('KEYWORDS:', kw_accuracy)\n",
    "print('PUNC NUMS:', punc_nums_accuracy)\n",
    "print('SIMPLE: ', simple_accuracy)\n",
    "print('SIMPLE BIGRAM: ', simple_bigram_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (.env)",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
