{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.tasks import process_deep_entries_data\n",
    "from core.helpers.common import (\n",
    "    rm_punc_not_nums, rm_punc_not_nums_list,\n",
    "    rm_stop_words_txt, rm_stop_words_txt_list,\n",
    "    translate_to_english_txt,\n",
    "    compose\n",
    ")\n",
    "from core.feature_selectors import UnigramFeatureSelector, BigramFeatureSelector\n",
    "from core.classifiers.NaiveBayes_classifier import NaiveBayesClassifier\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import random\n",
    "from nltk.corpus import names, movie_reviews\n",
    "import langid   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DEEP ENTRIES DATA\n",
      "DONE\n",
      "REMOVING PUNC AND STOP WORDS\n",
      "DONE\n",
      "SHUFFLING DATA\n",
      "DONE\n",
      "TAKING OUT TEST/TRAIN DATA\n",
      "length of training data 9326\n",
      "DONE\n",
      "COUNTING TAG FREQUENCIES in TRAIN DATA\n",
      "{'WASH': 716, 'Livelihood': 682, 'Education': 410, 'NFI': 454, 'Protection': 1793, 'Food': 1546, 'Shelter': 926, 'Cross': 407, 'Health': 1405, 'Logistic': 268, 'Agriculture': 350, 'Nutrition': 369}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = '_playground/sample_data/nlp_out.csv'\n",
    "\n",
    "print('PROCESSING DEEP ENTRIES DATA')\n",
    "data = process_deep_entries_data(csv_file_path)[:15000]\n",
    "print('DONE')\n",
    "\n",
    "print('REMOVING PUNC AND STOP WORDS')\n",
    "stemmer = PorterStemmer()\n",
    "rm_punc_and_stop = compose(\n",
    "    rm_punc_not_nums_list,\n",
    "    rm_stop_words_txt_list,\n",
    "    lambda x: list(map(str.lower, x))\n",
    "    #stemmer.stem # comment this if we don't need stemming\n",
    ")\n",
    "#rm_punc_and_stop = lambda x: x\n",
    "data = [(rm_punc_and_stop(str(ex).split()), l) for (ex, l) in data if langid.classify(str(ex))[0] == 'en']\n",
    "print('DONE')\n",
    "\n",
    "#data = [(list(movie_reviews.words(fileid)), category)\n",
    "#       for category in movie_reviews.categories()\n",
    "#      for fileid in movie_reviews.fileids(category)\n",
    "#]\n",
    "#print(data[0])\n",
    "tags_data = {}\n",
    "for ex, l in data:\n",
    "    tags_data[l] = tags_data.get(l, '') + \" \"+ str(ex)\n",
    "    \n",
    "all_tokenized_documents = list(map(lambda x:x.split(), [v for k, v in tags_data.items()]))\n",
    "\n",
    "print('SHUFFLING DATA')\n",
    "random.shuffle(data)\n",
    "print('DONE')\n",
    "\n",
    "data_len = len(data)\n",
    "test_len = int(data_len * 0.25)\n",
    "\n",
    "print('TAKING OUT TEST/TRAIN DATA')\n",
    "train_data = data[test_len:]\n",
    "print(\"length of training data\", len(train_data))\n",
    "test_data = data[:test_len]\n",
    "print('DONE')\n",
    "\n",
    "print('COUNTING TAG FREQUENCIES in TRAIN DATA')\n",
    "d = {}\n",
    "for ex, l in train_data:\n",
    "    d[l] = d.get(l, 0) + 1\n",
    "print(d)\n",
    "print('DONE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING FEATURE SELECTOR\n",
      "DONE\n",
      "CREATING CLASSIFIER\n",
      "DONE\n",
      "CALCULATING ACCURACY\n",
      "0.5791505791505791\n"
     ]
    }
   ],
   "source": [
    "print('CREATING FEATURE SELECTOR')\n",
    "from core.tf_idf import relevant_terms\n",
    "#most_relevant_terms = list(relevant_terms(all_tokenized_documents))\n",
    "#selector = UnigramFeatureSelector.new(freq_words=most_relevant_terms)\n",
    "selector = UnigramFeatureSelector.new(corpus=data, top=2000) # use top 2000 words\n",
    "print('DONE')\n",
    "\n",
    "# print('CREATING BIGRAM FEATURE SELECTOR')\n",
    "# selector = BigramFeatureSelector.new(corpus=data, top=2000)\n",
    "# selector = DocumentFeatureSelector.new(corpus=data, top=2000)\n",
    "# print('DONE')\n",
    "\n",
    "print('CREATING CLASSIFIER')\n",
    "classifier = NaiveBayesClassifier.new(selector, rm_punc_and_stop, train_data)\n",
    "print('DONE')\n",
    "\n",
    "print('CALCULATING ACCURACY')\n",
    "print(classifier.get_accuracy(test_data))\n",
    "\n",
    "#print('CONFUSION MATRIX')\n",
    "#print(classifier.get_confusion_matrix(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            |   A                                             |\n",
      "            |   g                   L               P         |\n",
      "            |   r       E           i           N   r         |\n",
      "            |   i       d           v   L       u   o         |\n",
      "            |   c       u           e   o       t   t   S     |\n",
      "            |   u       c       H   l   g       r   e   h     |\n",
      "            |   l   C   a       e   i   i       i   c   e     |\n",
      "            |   t   r   t   F   a   h   s       t   t   l   W |\n",
      "            |   u   o   i   o   l   o   t   N   i   i   t   A |\n",
      "            |   r   s   o   o   t   o   i   F   o   o   e   S |\n",
      "            |   e   s   n   d   h   d   c   I   n   n   r   H |\n",
      "------------+-------------------------------------------------+\n",
      "Agriculture | <57>  .   3   9   .   5   4   .   2   2   7   4 |\n",
      "      Cross |   7 <28>  4  10   2   2   7   9   5  19  20  10 |\n",
      "  Education |   .   3 <98>  4   4   5   4   1   .  12   5   5 |\n",
      "       Food |  39  14   6<246> 11  24  22   8  20  30  21  26 |\n",
      "     Health |   1   8   8   5<322>  3  10   3   9  46  15  26 |\n",
      " Livelihood |  44  10   4  32   6 <41>  5   6  12  20  14  12 |\n",
      "   Logistic |   3   5   2   4   5   . <48>  1   2  12   8   2 |\n",
      "        NFI |   5   3   3  10   4   2  17 <37>  6  11  39  22 |\n",
      "  Nutrition |   4   2   .   9  11   3   3   . <67> 11   .   4 |\n",
      " Protection |   1  12  19   4  13   6  10  14  10<564> 25   9 |\n",
      "    Shelter |   7   9  20   4   6   7  12  60   5  33<151>  7 |\n",
      "       WASH |  14   6   5   8  22   8  14   5   4   7  14<142>|\n",
      "------------+-------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classifier.get_confusion_matrix(test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (.env)",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
