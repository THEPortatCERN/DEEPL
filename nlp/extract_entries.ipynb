{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "import Ent\n",
    "\n",
    "#first run python manage.py shell_plus --notebook and   'django_extensions' is in INSTALLED_APPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "l = pickle.load( open( \"deep_pickle.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "grab entry data from DEEP\n",
    "\n",
    "----\n",
    "\n",
    "context: list\n",
    "population profile: list\n",
    "communication: list\n",
    "humanitarian access: list\n",
    "sector, pillar + subpillar: dict w/ lists... {sector: [subsect]...}\n",
    "reliability: int\n",
    "severity: int\n",
    "demo groups: list\n",
    "specific needs: list\n",
    "affected groups: list\n",
    "geo locations: list (blank for now)\n",
    "subsector: dict w/ lists... {sector: [subsect]...}\n",
    "information date: date\n",
    "excerpt: string\n",
    "has_image: T/F\n",
    "lead: int\n",
    "\"\"\"\n",
    "\n",
    "from entries.models import Entry\n",
    "from entries.models import Sector\n",
    "\n",
    "def extract():\n",
    "    \"\"\"iterate through each entry and extract information\"\"\"\n",
    "    deep_ents = []\n",
    "    \n",
    "    for ent_it in Entry.objects.all():\n",
    "\n",
    "        #pillar/subpillar/subsector. if just top row, second two are None\n",
    "\n",
    "        #context, population, communication, humanitarian access, pillar/subpillar\n",
    "        for subent in ent_it.entryinformation_set.all():\n",
    "            cent = Ent()\n",
    "\n",
    "            for info_att in subent.informationattribute_set.all():\n",
    "                #pillar/subpillar\n",
    "                if info_att.sector:\n",
    "                    cent.add_twodim(info_att)\n",
    "\n",
    "                #context, population, communication, humanitarian access\n",
    "                else:\n",
    "                    cent.add_onedim(info_att)\n",
    "\n",
    "            deep_ents.append(cent)\n",
    "\n",
    "            #reliability\n",
    "            cent.reliability = subent.reliability.name\n",
    "\n",
    "            #severity\n",
    "            cent.severity = subent.reliability.name\n",
    "\n",
    "            #demograhpic groups\n",
    "            if len(subent.vulnerable_groups.all()) > 0:\n",
    "                cent.demo_groups = [n.name for n in subent.vulnerable_groups.all()]\n",
    "\n",
    "            #specific\n",
    "            if len(subent.specific_needs_groups.all()) > 0:\n",
    "                cent.specific_needs = [n.name for n in subent.specific_needs_groups.all()]\n",
    "\n",
    "            #affected\n",
    "            if len(s.affected_groups.all()) > 0:\n",
    "                cent.affected_groups = [g.name for g in subent.affected_groups.all()]\n",
    "\n",
    "            #geo - none for now\n",
    "\n",
    "            #information date\n",
    "                cent.information_date = subent.date\n",
    "\n",
    "            #excerpt\n",
    "                cent.excerpt = subent.excerpt\n",
    "\n",
    "            #has_image\n",
    "                cent.has_image = subent.image == ''\n",
    "\n",
    "            #lead id\n",
    "                cent.lead = ent_it.lead_id\n",
    "\n",
    "            #lead content (passed for now to save space)\n",
    "                #cent.lead = ent_it.lead.simplifiedlead.text\n",
    "\n",
    "            #event\n",
    "            cent.event = ent_it.lead.event.name\n",
    "        \n",
    "    return deep_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle\n",
    "import pickle \n",
    "\n",
    "def pickle_rick():\n",
    "    #open with pickle.load( open( \"save.p\", \"rb\" ) )\n",
    "    prepare_to_get_ricked = extract()\n",
    "    pickle.dump(prepare_to_get_ricked, open('./deep_pickle.p', 'wb'))\n",
    "    \n",
    "pickle_rick()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-caea7be465e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#get lookups for sectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mSECT_LOOK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "#data input\n",
    "\"\"\"\n",
    "context: list\n",
    "population profile: list\n",
    "communication: list\n",
    "humanitarian access: list\n",
    "sector, pillar + subpillar: dict w/ lists... {sector: [subsect]...}\n",
    "reliability: int\n",
    "severity: int\n",
    "demo groups: list\n",
    "specific needs: list\n",
    "affected groups: list\n",
    "geo locations: list\n",
    "subsector: dict w/ lists... {sector: [subsect]...}\n",
    "information date: date\n",
    "excerpt: string\n",
    "has_image: \n",
    "\"\"\"\n",
    "from entries.models import Entry\n",
    "from entries.models import Sector\n",
    "\n",
    "#get lookups for sectors\n",
    "SECT_LOOK = {i.id : i.name for i in models.Sector.objects.all()}\n",
    "\n",
    "\n",
    "def extract_sects(raw):\n",
    "\tsect_dict = {}\n",
    "\tfor v in raw:\n",
    "\t\t#there can be multiple sectors in an entry so we have to cycle through and get them\n",
    "\t\t#since we can have sub-pillars, we take the unique sector types\n",
    "\n",
    "\t\tfor uniq in list(set(SECT_LOOK[k['sector_id']] for k in v.informationattribute_set.values())):\n",
    "\t\t\tif v.pk not in sect_dict:\n",
    "\t\t\t\tsect_dict[v.pk] = {v.excerpt : [uniq]}\n",
    "\t\t\telse:\n",
    "\t\t\t\tsect_dict[v.pk][v.excerpt].append(uniq)\n",
    "\n",
    "\treturn sect_dict\n",
    "\n",
    "def create_csv(data, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        for k,v in data.items():\n",
    "            csv_writer.writerow([k, v])\n",
    "\n",
    "l = [i.entryinformation_set.all() for i in Entry.objects.all()]\n",
    "div = [sub for list in l for sub in list]\n",
    "\n",
    "extract_sects(div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data prep\n",
    "\n",
    "#read in csv, make dict with cats\n",
    "#structure: 0 country | 1 date | 2 sect | 3 subsect | 4 text\n",
    "\n",
    "rd = csv.reader(open('/Users/ewanog/code/repos/deep/out'))\n",
    "\n",
    "cats = {}\n",
    "\n",
    "for row in rd:\n",
    "    sect = row[0]\n",
    "    txt = row[1:]\n",
    "    if sect not in cats:\n",
    "        cats[sect] = txt\n",
    "    else:\n",
    "        cats[sect].extend(txt)\n",
    "\n",
    "#counts\n",
    "c=0\n",
    "for k,v in cats.items():\n",
    "    print(k + ' ' + str(len(v)))\n",
    "    c+=len(v)\n",
    "print('total count: ' + str(c))\n",
    "    \n",
    "print('all: ' + str(len(cats.values())))\n",
    "    \n",
    "cats    \n",
    "    \n",
    "#TODO: clean bad chars\n",
    "\n",
    "#remove stopwords. STOP!!!!!!!!!ADSAAA!!W\n",
    "\n",
    "#lower - then remove all the other calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#show duplicates\n",
    "from pprint import pprint\n",
    "\n",
    "c=0\n",
    "for k,v in cats.items():\n",
    "    print(k + ' ' + str(len((set(v)))))\n",
    "    c+=len((set(v)))\n",
    "           \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create random sampling groups\n",
    "#TODO: should we do uniform random sampling per group or a whole scale random draw?\n",
    "\n",
    "#return dicts with {sect: setted list of entries}\n",
    "def gen_tt(corpus):\n",
    "    test = {}\n",
    "    train = {}\n",
    "\n",
    "    #add corpus to test and train\n",
    "    for k in corpus.keys():\n",
    "        test[k] = None\n",
    "        train[k] = None\n",
    "\n",
    "    for k,v in corpus.items():\n",
    "        random.shuffle(v)\n",
    "\n",
    "        setv = list(set(v))\n",
    "        len_test = int(len(setv)*.3)\n",
    "\n",
    "        test[k] = setv[:len_test]\n",
    "        train[k] = setv[len_test:]\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train, test = gen_tt(cats)\n",
    "#     #check lengths\n",
    "#     for k in train.keys():\n",
    "#         print(k + ' train : ' + str(len(train[k])))\n",
    "#         print(k + ' test : ' + str(len(test[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test sampling (if we do 70:30)\n",
    "\n",
    "d = {\n",
    "    'cat1': [random.randint(0,100) for i in range(100)],\n",
    "    'cat2': [random.randint(0,100) for i in range(150)]\n",
    "}\n",
    "\n",
    "test_test = {}\n",
    "test_train = {}\n",
    "\n",
    "#add cats to test and train\n",
    "for k in d.keys():\n",
    "    test_test[k] = None\n",
    "    test_train[k] = None\n",
    "\n",
    "for k,v in d.items():\n",
    "    len_test = int(len(v)*.3)\n",
    "    print(len(v)*.3)\n",
    "    \n",
    "    test_test[k] = v[:len_test]\n",
    "    test_train[k] = v[len_test:]\n",
    "    \n",
    "print(len(test_test))\n",
    "print(len(test_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start building our model\n",
    "#train and test are just subsets of cats (for now)\n",
    "\n",
    "#make a feature with word presence\n",
    "#TODO: what's python func?\n",
    "all_words = []\n",
    "for k,v in cats.items():\n",
    "    for e in list(set(v)):\n",
    "        for w in e.split(' '):\n",
    "            all_words.append(w.lower())\n",
    "\n",
    "freq_words = nltk.FreqDist(all_words)\n",
    "\n",
    "def document_features(document):\n",
    "    if type(document) == list:\n",
    "        #TODO: func\n",
    "        grp = ''\n",
    "        for l in document:\n",
    "            for w in l:\n",
    "                grp += w\n",
    "        document = grp\n",
    "                \n",
    "    uniq_doc = set(document.split(' '))\n",
    "    features = {}\n",
    "    for word in list(freq_words.keys())[:2000]:\n",
    "        features['contains(%s)' % word] = (word in uniq_doc)\n",
    "    return features\n",
    "\n",
    "#convert dicts into list of tuples we need\n",
    "#TODO: func\n",
    "train_merge = []\n",
    "test_merge = []\n",
    "\n",
    "for k,v in train.items():\n",
    "    for i in v:\n",
    "        train_merge.append((k,i))\n",
    "\n",
    "for k,v in test.items():\n",
    "    for i in v:\n",
    "        test_merge.append((k,i))\n",
    "\n",
    "        \n",
    "print(len(test_merge))\n",
    "\n",
    "wp_train = [(document_features(v), k) for k,v in train_merge]\n",
    "wp_test = [(document_features(v), k) for k,v in test_merge]\n",
    "\n",
    "nb_class = nltk.NaiveBayesClassifier.train(wp_train)\n",
    "\n",
    "# wp_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#results\n",
    "\n",
    "#from just raw, unrefined features and 200 words: 0.398576512455516\n",
    "\n",
    "nb_class.show_most_informative_features(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ['contains(hit)',\n",
    "#  'contains(down)',\n",
    "#  'contains(being)',\n",
    "#  'contains(member.)',\n",
    "#  'contains(17)',\n",
    "#  'contains(other)',\n",
    "#  'contains(air)',\n",
    "#  'contains(redirect,)',\n",
    "#  'contains(released)',\n",
    "#  'contains(independence)',\n",
    "#  'contains(until)',\n",
    "#  'contains(the)']\n",
    "\n",
    "# [k for k,v in wp_train[100][0].items() if v is True]\n",
    "\n",
    "wp_train[100][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k,v in train.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wpfeatures[0][0]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
